# -*- coding: utf-8 -*-
"""Git CV-C3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EEfBI-09gi9go0VcKyV6jkBOeIJASkMp
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install albumentations opencv-python-headless segmentation-models-pytorch

"""
Pupil & Iris Segmentation Pipeline

Architecture: U-Net++ with ResNet34 encoder

Author: Daniel Nguyen
"""

import os
import numpy as np
import pandas as pd
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

torch.manual_seed(42)
np.random.seed(42)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")

# Configuration class
class Config:
    def __init__(self, base_dir='.'):
        """
        Sets up dataset paths and training parameters.

        Args:
            base_dir (str): main folder containing your dataset
        """
        self.BASE_DIR = base_dir

        # Dataset locations
        self.TRAIN_IMG_DIR = os.path.join(self.BASE_DIR, 'training_set', 'images')
        self.TRAIN_GT_DIR = os.path.join(self.BASE_DIR, 'training_set', 'groundtruth')
        self.TEST_IMG_DIR = os.path.join(self.BASE_DIR, 'testing_set', 'images')
        self.TEST_GT_DIR = os.path.join(self.BASE_DIR, 'testing_set', 'groundtruth')

        # Image settings
        self.IMAGE_HEIGHT = 160
        self.IMAGE_WIDTH = 224
        self.IN_CHANNELS = 3
        self.NUM_CLASSES = 3  # 0: background, 1: pupil, 2: iris

        # Training parameters
        self.BATCH_SIZE = 16
        self.NUM_EPOCHS = 120
        self.LR = 1e-3
        self.WEIGHT_DECAY = 1e-5

        # Early stopping
        self.PATIENCE = 15
        self.MIN_DELTA = 1e-4

        # Model checkpoint
        self.MODEL_SAVE_PATH = os.path.join(self.BASE_DIR, 'best_pupil_iris_model.pth')

# set the base path
base_path = '/path/to/data' # this may change
cfg = Config(base_dir=base_path)

class PupilIrisDataset(Dataset):
    """Dataset for eye images and pupil/iris segmentation masks."""

    def __init__(self, img_dir, gt_dir, transform=None):
        self.img_dir = img_dir
        self.gt_dir = gt_dir
        self.transform = transform

        # get all image files and sort them
        self.imgs = []
        for f in os.listdir(img_dir):
            if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                self.imgs.append(f)
        self.imgs.sort()



        self.gts = []
        for f in os.listdir(gt_dir):
            if f.lower().endswith('.csv'):
                self.gts.append(f)
        self.gts.sort()

        # make sure we have matching files
        assert len(self.imgs) > 0, f"No images found in {img_dir}"
        assert len(self.imgs) == len(self.gts), "Image and CSV counts don't match"

    def __len__(self):
        return len(self.imgs)

    def __getitem__(self, idx):
        # load the image and convert to RGB
        img_path = os.path.join(self.img_dir, self.imgs[idx])
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w = img.shape[:2]


        gt_path = os.path.join(self.gt_dir, self.gts[idx])
        gt = pd.read_csv(gt_path).iloc[0]

        # extract pupil ellipse params
        pupil = (gt['coord_p_true_x'], gt['coord_p_true_y'],
                 gt['radiusX_p_true'], gt['radiusY_p_true'], gt['theta_p_true'])

        # iris
        iris = (gt['coord_i_true_x'], gt['coord_i_true_y'],
                gt['radiusX_i_true'], gt['radiusY_i_true'], gt['theta_i_true'])

        # draw elipse masks
        pupil_mask = self._draw_ellipse(h, w, *pupil)
        iris_mask = self._draw_ellipse(h, w, *iris)

        # create segmentation mask: 0=background, 1=pupil, 2=iris
        seg = np.zeros((h, w), dtype=np.uint8)
        seg[iris_mask == 1] = 2      # fill iris region with 2
        seg[pupil_mask == 1] = 1     # overwrite pupil region with 1

        # apply transforms
        if self.transform:
            augmented = self.transform(image=img, mask=seg)
            img = augmented['image']
            seg = augmented['mask']
        else:
            # convert to tensor
            img = ToTensorV2()(image=img)['image']
            seg = torch.tensor(seg, dtype=torch.long)

        # return dict
        metadata = {
            'filename': self.imgs[idx],
            'pupil': pupil,
            'iris': iris
        }
        return img, seg, metadata

    def _draw_ellipse(self, h, w, cx, cy, rx, ry, theta):
        """Helper function to draw a filled ellipse on a blank mask."""
        mask = np.zeros((h, w), dtype=np.uint8)

        # cv2.ellipse needs integer pixel coordinates
        center = (int(round(cx)), int(round(cy)))
        axes = (int(round(rx)), int(round(ry)))
        angle = np.degrees(theta)  # change to degrees

        # draw filled ellipse (thickness=-1 means filled)
        cv2.ellipse(mask, center, axes, angle,
                   startAngle=0, endAngle=360,
                   color=1, thickness=-1)

        return mask

def get_train_transforms():
    """Augmentation pipeline for training data."""

    # start with resize and padding to match target dimensions
    transforms_list = [
        A.LongestMaxSize(max_size=cfg.IMAGE_WIDTH),
        A.PadIfNeeded(min_height=cfg.IMAGE_HEIGHT,
                     min_width=cfg.IMAGE_WIDTH,
                     border_mode=cv2.BORDER_CONSTANT)
    ]

    # add geometric augmentations
    transforms_list.append(A.Rotate(limit=15, p=0.3))
    transforms_list.append(A.ShiftScaleRotate(shift_limit=0.05,
                                              scale_limit=0.15,
                                              rotate_limit=15,
                                              p=0.5,
                                              border_mode=cv2.BORDER_CONSTANT))


    # add color/brightness augmentations
    # OneOf will randomly pick one of these to apply
    color_transforms = A.OneOf([
        A.RandomGamma(gamma_limit=(80, 120), p=0.3),
        A.RandomBrightnessContrast(brightness_limit=0.2,
                                  contrast_limit=0.2, p=0.3)
    ], p=0.5)
    transforms_list.append(color_transforms)

    # add more color adjustments
    transforms_list.append(A.HueSaturationValue(hue_shift_limit=10,
                                               sat_shift_limit=20,
                                               val_shift_limit=20,
                                               p=0.3))

    transforms_list.append(A.GaussianBlur(blur_limit=(3, 5), p=0.3))
    transforms_list.append(A.CLAHE(p=0.3))

    # normalize using ImageNet stats and convert to tensor
    transforms_list.append(A.Normalize(mean=[0.485, 0.456, 0.406],
                                      std=[0.229, 0.224, 0.225]))
    transforms_list.append(ToTensorV2())

    return A.Compose(transforms_list)


def get_val_transforms():
    """Validation transforms with no random augmentations."""

    # just resize, pad, normalize, and tensorize
    val_transforms = []
    val_transforms.append(A.LongestMaxSize(max_size=cfg.IMAGE_WIDTH))
    val_transforms.append(A.PadIfNeeded(min_height=cfg.IMAGE_HEIGHT,
                                       min_width=cfg.IMAGE_WIDTH,
                                       border_mode=cv2.BORDER_CONSTANT))
    val_transforms.append(A.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225]))
    val_transforms.append(ToTensorV2())

    return A.Compose(val_transforms)

class ConvBlock(nn.Module):
    """Basic building block with two conv layers."""

    def __init__(self, in_ch, out_ch):
        super().__init__()

        # two 3x3 convolutions with batch norm and relu
        # bias=False because batch norm handles the bias term
        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_ch)
        self.relu1 = nn.ReLU(inplace=True)

        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_ch)
        self.relu2 = nn.ReLU(inplace=True)

    def forward(self, x):
        # pass through both conv layers
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu2(x)

        return x


class UNetPlusPlus(nn.Module):
    """UNet++ model for segmenting pupil and iris from eye images."""

    def __init__(self, in_ch=3, num_classes=3, dropout=0.2):
        super().__init__()

        # load pretrained ResNet34
        resnet = models.resnet34(pretrained=True)

        # encoder layers from ResNet34
        # enc0 is the initial conv layer
        if in_ch == 3:
            self.enc0 = resnet.conv1
        else:
            # if we have different number of input channels, create new conv layer
            self.enc0 = nn.Conv2d(in_ch, 64, kernel_size=7, stride=2,
                                 padding=3, bias=False)

        self.bn0 = resnet.bn1
        self.relu = resnet.relu
        self.maxpool = resnet.maxpool

        # ResNet blocks
        self.enc1 = resnet.layer1  # 64 channels
        self.enc2 = resnet.layer2  # 128 channels
        self.enc3 = resnet.layer3  # 256 channels
        self.enc4 = resnet.layer4  # 512 channels

        # decoder upsampling layers
        # bilinear interpolation to double the spatial dimensions
        self.up4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.up3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)

        # decoder conv blocks
        self.dec3_1 = ConvBlock(256 + 512, 256)  # enc3 + upsampled enc4
        self.dec2_1 = ConvBlock(128 + 256, 128)  # enc2 + upsampled dec3_1
        self.dec1_1 = ConvBlock(64 + 128, 64)    # enc1 + upsampled dec2_1
        self.dec0_1 = ConvBlock(64 + 64, 64)

        # optional dropout
        if dropout > 0:
            self.dropout = nn.Dropout2d(dropout)
        else:
            self.dropout = nn.Identity()

        # final 1x1 conv to get class predictions
        self.final = nn.Conv2d(64, num_classes, kernel_size=1)

    def forward(self, x):
        # save original size to resize output later if needed
        orig_h, orig_w = x.shape[2], x.shape[3]

        # encoder pass
        x0_0 = self.enc0(x)
        x0_0 = self.bn0(x0_0)
        x0_0 = self.relu(x0_0)

        x1_0 = self.maxpool(x0_0)
        x1_0 = self.enc1(x1_0)

        x2_0 = self.enc2(x1_0)
        x3_0 = self.enc3(x2_0)
        x4_0 = self.enc4(x3_0)

        # decoder pass with skip connections
        # concatenate encoder features with upsampled decoder features
        up4 = self.up4(x4_0)
        cat3 = torch.cat([x3_0, up4], dim=1)
        d3_1 = self.dec3_1(cat3)

        up3 = self.up3(d3_1)
        cat2 = torch.cat([x2_0, up3], dim=1)
        d2_1 = self.dec2_1(cat2)

        up2 = self.up2(d2_1)
        cat1 = torch.cat([x1_0, up2], dim=1)
        d1_1 = self.dec1_1(cat1)

        up1 = self.up1(d1_1)
        cat0 = torch.cat([x0_0, up1], dim=1)
        d0_1 = self.dec0_1(cat0)

        # apply dropout
        d0_1 = self.dropout(d0_1)
        out = self.final(d0_1)

        # resize to original input if diff
        out_h, out_w = out.shape[2], out.shape[3]
        if out_h != orig_h or out_w != orig_w:
            out = F.interpolate(out, size=(orig_h, orig_w),
                              mode='bilinear', align_corners=False)

        return out

class WeightedDiceLoss(nn.Module):
    """Dice loss with different weights for each class."""

    def __init__(self, weights=None, smooth=1.0):
        super().__init__()
        self.smooth = smooth

        # give pupil higher weight since it's smaller, worse performance on it
        if weights is not None:
            self.weights = weights
        else:
            self.weights = [1.0, 2.0, 1.0]  # background, pupil, iris

    def forward(self, prediction, target_mask):
        num_classes = prediction.shape[1]

        # resize prediction to match target if needed
        pred_h, pred_w = prediction.shape[2], prediction.shape[3]
        target_h, target_w = target_mask.shape[1], target_mask.shape[2]
        if pred_h != target_h or pred_w != target_w:
            prediction = F.interpolate(prediction,
                                      size=(target_h, target_w),
                                      mode='bilinear',
                                      align_corners=False)

        # convert target to one hot encoding
        target_onehot = F.one_hot(target_mask.long(), num_classes)
        target_onehot = target_onehot.permute(0, 3, 1, 2).float()


        prediction_probs = F.softmax(prediction, dim=1)

        # calculate dice
        dice_scores = []
        for c in range(num_classes):
            pred_c = prediction_probs[:, c]
            target_c = target_onehot[:, c]

            # i and u
            intersection = pred_c * target_c
            intersection = intersection.sum(dim=(1, 2))


            pred_sum = pred_c.sum(dim=(1, 2))
            target_sum = target_c.sum(dim=(1, 2))
            union = pred_sum + target_sum

            # formula
            dice = (2.0 * intersection + self.smooth) / (union + self.smooth)

            # weight this class
            weighted_dice = dice * self.weights[c]
            dice_scores.append(weighted_dice)

        # stack all class dice scores and take mean
        all_dice = torch.stack(dice_scores, dim=1)
        mean_dice = all_dice.mean()

        # return loss (1 - dice since we want to minimize)
        return 1.0 - mean_dice


class FocalLoss(nn.Module):
    """Focal loss to focus on hard examples."""

    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, prediction, target_mask):
        # make sure prediction and target have same size
        pred_h, pred_w = prediction.shape[2], prediction.shape[3]
        target_h, target_w = target_mask.shape[1], target_mask.shape[2]
        if pred_h != target_h or pred_w != target_w:
            prediction = F.interpolate(prediction,
                                      size=(target_h, target_w),
                                      mode='bilinear',
                                      align_corners=False)

        # get standard cross entropy loss
        ce_loss = F.cross_entropy(prediction, target_mask.long(), reduction='none')

        # get the probability of correct class
        pt = torch.exp(-ce_loss)

        # apply focal term: (1 - pt)^gamma
        # this down-weights easy examples and focuses on hard ones
        focal_term = (1 - pt) ** self.gamma
        focal_loss = self.alpha * focal_term * ce_loss

        return focal_loss.mean()


class CombinedLoss(nn.Module):
    """Combines Dice loss and Focal loss."""

    def __init__(self, dice_weight=0.7, focal_weight=0.3):
        super().__init__()

        # create the two loss functions
        self.dice_loss = WeightedDiceLoss(weights=[1.0, 2.0, 1.0])
        self.focal_loss = FocalLoss(alpha=0.25, gamma=2.0)

        # save weights
        self.dice_weight = dice_weight
        self.focal_weight = focal_weight

    def forward(self, prediction, target_mask):
        # calculate both losses
        dice = self.dice_loss(prediction, target_mask)
        focal = self.focal_loss(prediction, target_mask)

        # weighted combination
        total_loss = self.dice_weight * dice + self.focal_weight * focal

        return total_loss

def fit_ellipse_from_mask(mask, min_pts=5):
    """Fit an ellipse to a binary mask using OpenCV."""

    # make sure mask is the right data type for OpenCV
    mask_uint8 = mask.astype(np.uint8)

    # find contours in the mask
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL,
                                   cv2.CHAIN_APPROX_NONE)

    # check if we found any contours
    if len(contours) == 0:
        return None

    # find the biggest contour (should be our pupil or iris)
    biggest_contour = None
    biggest_area = 0
    for contour in contours:
        area = cv2.contourArea(contour)
        if area > biggest_area:
            biggest_area = area
            biggest_contour = contour

    # make sure we have enough points to fit an ellipse
    if len(biggest_contour) < min_pts:
        return None

    # try to fit the ellipse
    try:

        '''
        # fitEllipse returns ((cx, cy), (width, height), angle)
        '''
        ellipse = cv2.fitEllipse(biggest_contour)
        center = ellipse[0]
        axes = ellipse[1]
        angle = ellipse[2]

        # extract parameters
        cx = center[0]
        cy = center[1]
        radius_x = axes[0] / 2.0  # width to radius
        radius_y = axes[1] / 2.0  # height to radius
        theta = np.radians(angle)  # convert degrees to radians

        return cx, cy, radius_x, radius_y, theta

    except:
        # if fitEllipse fails, use image moments as backup
        M = cv2.moments(biggest_contour)


        # check for division by zero
        if M['m00'] == 0:
            return None

        # center from moments
        cx = M['m10'] / M['m00']
        cy = M['m01'] / M['m00']

        # approximate radii from second moments
        radius_x = np.sqrt(M['mu20'] / M['m00'])
        radius_y = np.sqrt(M['mu02'] / M['m00'])
        theta = 0.0  # can't estimate angle from moments easily

        return cx, cy, radius_x, radius_y, theta


def fit_ellipse_robust(mask):
    """Fit ellipse with some preprocessing to clean up the mask."""

    total_pixels = mask.sum()
    if total_pixels == 0:
        return None

    # clean up the mask with morphological operations
    # this removes small noise and fills small holes
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))

    # close operation fills small holes
    mask_clean = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)

    # open operation removes small noise
    mask_clean = cv2.morphologyEx(mask_clean, cv2.MORPH_OPEN, kernel)

    # now fit ellipse on the cleaned mask
    result = fit_ellipse_from_mask(mask_clean)

    return result

def iou(mask1, mask2):
    """Calculate Intersection over Union between two binary masks."""

    intersection = np.logical_and(mask1, mask2)
    intersection_count = intersection.sum()

    union = np.logical_or(mask1, mask2)
    union_count = union.sum()


    if union_count == 0:
        return 0.0

    # Formula
    iou_score = intersection_count / union_count
    return iou_score


def ellipse_to_mask(h, w, cx, cy, rx, ry, theta):
    """Draw an ellipse on a blank mask."""

    # create empty mask
    mask = np.zeros((h, w), dtype=np.uint8)

    # convert parameters to what cv2.ellipse expects
    center = (int(round(cx)), int(round(cy)))
    axes = (int(round(rx)), int(round(ry)))
    angle_deg = np.degrees(theta)

    # draw filled ellipse
    cv2.ellipse(mask, center, axes, angle_deg,
               startAngle=0, endAngle=360,
               color=1, thickness=-1)

    return mask


def compute_metrics(pred_params, gt_params, img_shape):
    """Compare predicted ellipses to ground truth ellipses."""

    h, w = img_shape

    # get predicted ellipse parameters
    pred_pupil = pred_params['pupil']
    pred_iris = pred_params['iris']


    # diameter is 2 * radius, we use y-axis radius
    pred_pupil_diam = 2 * pred_pupil[3]
    gt_pupil_diam = 2 * gt_params['pupil_radius_y']
    pupil_diam_err = abs(pred_pupil_diam - gt_pupil_diam) / gt_pupil_diam

    pred_iris_diam = 2 * pred_iris[3]
    gt_iris_diam = 2 * gt_params['iris_radius_y']
    iris_diam_err = abs(pred_iris_diam - gt_iris_diam) / gt_iris_diam

    # create masks for predicted pupil and iris
    pred_p_mask = ellipse_to_mask(h, w, *pred_pupil)
    pred_i_mask = ellipse_to_mask(h, w, *pred_iris)

    # create masks for ground truth pupil
    gt_p_mask = ellipse_to_mask(h, w,
                                gt_params['pupil_center_x'],
                                gt_params['pupil_center_y'],
                                gt_params['pupil_radius_x'],
                                gt_params['pupil_radius_y'],
                                gt_params['pupil_theta'])

    # create mask for ground truth iris
    gt_i_mask = ellipse_to_mask(h, w,
                                gt_params['iris_center_x'],
                                gt_params['iris_center_y'],
                                gt_params['iris_radius_x'],
                                gt_params['iris_radius_y'],
                                gt_params['iris_theta'])

    pupil_iou_score = iou(pred_p_mask, gt_p_mask)
    iris_iou_score = iou(pred_i_mask, gt_i_mask)


    metrics = {
        'pupil_diameter_error': pupil_diam_err,
        'iris_diameter_error': iris_diam_err,
        'pupil_iou': pupil_iou_score,
        'iris_iou': iris_iou_score
    }

    return metrics

def train_epoch(model, loader, criterion, optimizer, device):
    """Train the model for one epoch."""

    # set model to training mode
    model.train()

    # keep track of losses
    epoch_losses = []


    for imgs, masks, _ in tqdm(loader, desc="Training", leave=False):
        # move data to GPU if available
        imgs = imgs.to(device)
        masks = masks.to(device)

        # forward pass: get predictions
        preds = model(imgs)


        loss = criterion(preds, masks)

        # backward pass: compute gradients
        optimizer.zero_grad()  # clear old gradients
        loss.backward()  # compute new gradients
        optimizer.step()  # update weights

        # save
        epoch_losses.append(loss.item())

    # calculate average loss across all batches
    avg_loss = np.mean(epoch_losses)

    return avg_loss


def validate_epoch(model, loader, criterion, device):
    """Validate the model for one epoch."""

    # set model to evaluation mode (disables dropout, etc)
    model.eval()


    val_losses = []

    with torch.no_grad():
        for imgs, masks, _ in tqdm(loader, desc="Validation", leave=False):
            # move data to GPU if available
            imgs = imgs.to(device)
            masks = masks.to(device)

            # get predictions
            preds = model(imgs)

            # calculate loss
            loss = criterion(preds, masks)

            # save this batch's loss
            val_losses.append(loss.item())

    # calculate average validation loss
    avg_loss = np.mean(val_losses)

    return avg_loss

def predict_image(model, img_path, transform, device):
    """Run model on a single image and extract pupil/iris ellipses."""


    img = cv2.imread(img_path)
    if img is None:
        raise FileNotFoundError(f"Could not load image: {img_path}")


    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w = img.shape[:2]

    # preprocess the image
    transformed = transform(image=img)
    img_tensor = transformed['image']

    # add batch dimension and move to device
    img_tensor = img_tensor.unsqueeze(0)
    img_tensor = img_tensor.to(device)

    # run inference
    model.eval()
    with torch.no_grad():
        logits = model(img_tensor)

        # get class predictions (argmax over channel dimension)
        pred_mask = torch.argmax(logits, dim=1)
        pred_mask = pred_mask.squeeze()
        pred_mask = pred_mask.cpu().numpy()

    # extract individual masks
    pupil_mask = (pred_mask == 1).astype(np.uint8)
    iris_mask = (pred_mask == 2).astype(np.uint8)

    # fit ellipses to the masks
    pupil_ellipse = fit_ellipse_robust(pupil_mask)
    iris_ellipse = fit_ellipse_robust(iris_mask)

    # if pupil ellipse fitting failed, try simple moment-based estimate
    if pupil_ellipse is None:
        if pupil_mask.sum() > 0:
            M = cv2.moments(pupil_mask)
            if M['m00'] > 0:
                # get center from moments
                cx = M['m10'] / M['m00']
                cy = M['m01'] / M['m00']

                # if we have iris, make sure pupil is inside it
                if iris_ellipse is not None:
                    cx = np.clip(cx,
                               iris_ellipse[0] - iris_ellipse[2],
                               iris_ellipse[0] + iris_ellipse[2])
                    cy = np.clip(cy,
                               iris_ellipse[1] - iris_ellipse[3],
                               iris_ellipse[1] + iris_ellipse[3])

                # estimate radius from area
                r = np.sqrt(M['m00'] * 0.25)
                r = np.clip(r, 2, 50)


                pupil_ellipse = (cx, cy, r, r, 0.0)

    # if iris ellipse fitting failed, try simple moment-based estimate
    if iris_ellipse is None:
        if iris_mask.sum() > 0:
            M = cv2.moments(iris_mask)
            if M['m00'] > 0:
                # get center from moments
                cx = M['m10'] / M['m00']
                cy = M['m01'] / M['m00']

                # estimate radius from area
                r = np.sqrt(M['m00'] * 0.25)
                r = np.clip(r, 5, 80)
                iris_ellipse = (cx, cy, r, r, 0.0)


    results = {
        'pupil': pupil_ellipse,
        'iris': iris_ellipse,
        'pupil_mask': pupil_mask,
        'iris_mask': iris_mask
    }

    return results

class PupilIrisPredictor:
    """Class to load a trained model and run predictions on images."""

    def __init__(self, model_path, device=None):

        if device is None:
            if torch.cuda.is_available():
                self.device = torch.device('cuda')
            else:
                self.device = torch.device('cpu')
        else:
            self.device = device

        # create the model
        self.model = UNetPlusPlus(in_ch=3, num_classes=3)

        # load trained weights
        state_dict = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(state_dict)

        # move model to device and set to eval mode
        self.model.to(self.device)
        self.model.eval()

        # get transforms for preprocessing
        self.transform = get_val_transforms()

        print(f"Loaded model from {model_path} on {self.device}")

    def predict_single(self, img_path):
        """Run prediction on a single image."""
        result = predict_image(self.model, img_path, self.transform, self.device)
        return result

    def predict_dataset(self, img_dir, gt_dir=None, output_csv='predictions.csv'):
        """Run predictions on all images in a directory."""

        # get all image files and sort them
        img_files = []
        for f in os.listdir(img_dir):
            if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                img_files.append(f)
        img_files.sort()

        # store results for all images
        results = []
        metrics = []

        # loop through all images
        for img_name in tqdm(img_files, desc='Inference'):
            img_path = os.path.join(img_dir, img_name)

            pred = self.predict_single(img_path)

            # save predicted ellipse parameters
            result_row = {
                'filename': img_name
            }

            # add pupil parameters
            if pred['pupil'] is not None:
                result_row['coord_p_true_x'] = pred['pupil'][0]
                result_row['coord_p_true_y'] = pred['pupil'][1]
                result_row['radiusX_p_true'] = pred['pupil'][2]
                result_row['radiusY_p_true'] = pred['pupil'][3]
                result_row['theta_p_true'] = pred['pupil'][4]
            else:
                result_row['coord_p_true_x'] = None
                result_row['coord_p_true_y'] = None
                result_row['radiusX_p_true'] = None
                result_row['radiusY_p_true'] = None
                result_row['theta_p_true'] = None


            if pred['iris'] is not None:
                result_row['coord_i_true_x'] = pred['iris'][0]
                result_row['coord_i_true_y'] = pred['iris'][1]
                result_row['radiusX_i_true'] = pred['iris'][2]
                result_row['radiusY_i_true'] = pred['iris'][3]
                result_row['theta_i_true'] = pred['iris'][4]
            else:
                result_row['coord_i_true_x'] = None
                result_row['coord_i_true_y'] = None
                result_row['radiusX_i_true'] = None
                result_row['radiusY_i_true'] = None
                result_row['theta_i_true'] = None

            results.append(result_row)


            if gt_dir is not None:
                # build ground truth CSV filename
                img_basename = img_name.rsplit('.', 1)[0]
                gt_csv = os.path.join(gt_dir, img_basename + '.csv')

                if os.path.exists(gt_csv):
                    # load ground truth
                    gt_df = pd.read_csv(gt_csv)
                    gt = gt_df.iloc[0]

                    # extract ground truth parameters
                    gt_params = {
                        'pupil_center_x': gt['coord_p_true_x'],
                        'pupil_center_y': gt['coord_p_true_y'],
                        'pupil_radius_x': gt['radiusX_p_true'],
                        'pupil_radius_y': gt['radiusY_p_true'],
                        'pupil_theta': gt['theta_p_true'],
                        'iris_center_x': gt['coord_i_true_x'],
                        'iris_center_y': gt['coord_i_true_y'],
                        'iris_radius_x': gt['radiusX_i_true'],
                        'iris_radius_y': gt['radiusY_i_true'],
                        'iris_theta': gt['theta_i_true']
                    }

                    # get image shape for metrics calculation
                    img = cv2.imread(img_path)
                    img_shape = img.shape[:2]

                    # compute metrics
                    metric = compute_metrics(pred, gt_params, img_shape)
                    metric['filename'] = img_name
                    metrics.append(metric)

        # save predictions to CSV
        results_df = pd.DataFrame(results)
        results_df.to_csv(output_csv, index=False)
        print(f"\nSaved predictions to {output_csv}")


        metrics_df = None
        if len(metrics) > 0:
            metrics_df = pd.DataFrame(metrics)

            # calculate average metrics
            avg_pupil_diam_err = metrics_df['pupil_diameter_error'].mean()
            avg_iris_diam_err = metrics_df['iris_diameter_error'].mean()
            avg_pupil_iou = metrics_df['pupil_iou'].mean()
            avg_iris_iou = metrics_df['iris_iou'].mean()

            print("EVALUATION METRICS SUMMARY")
            print(f"Pupil Diameter Error: {avg_pupil_diam_err*100:.2f}%")
            print(f"Iris Diameter Error:  {avg_iris_diam_err*100:.2f}%")
            print(f"Pupil IoU:            {avg_pupil_iou*100:.2f}%")
            print(f"Iris IoU:             {avg_iris_iou*100:.2f}%")

        return results_df, metrics_df

def train_model():
    """Train the segmentation model on the dataset."""

    print("Segmentation Training")

    # load
    train_ds = PupilIrisDataset(cfg.TRAIN_IMG_DIR, cfg.TRAIN_GT_DIR,
                                transform=get_train_transforms())


    # split
    train_size = int(0.85 * len(train_ds))
    val_size = len(train_ds) - train_size
    train_subset, val_subset = torch.utils.data.random_split(train_ds,
                                                             [train_size, val_size])

    # create data loaders
    train_loader = DataLoader(train_subset,
                             batch_size=cfg.BATCH_SIZE,
                             shuffle=True,
                             num_workers=2,
                             pin_memory=True)

    val_loader = DataLoader(val_subset,
                           batch_size=cfg.BATCH_SIZE,
                           shuffle=False,
                           num_workers=2,
                           pin_memory=True)

    print(f"Train samples: {len(train_subset)} | Validation samples: {len(val_subset)}\n")

    # create model and move to device
    model = UNetPlusPlus(in_ch=cfg.IN_CHANNELS, num_classes=cfg.NUM_CLASSES)
    model = model.to(device)

    # setup loss function
    criterion = CombinedLoss(dice_weight=0.7, focal_weight=0.3)


    optimizer = torch.optim.AdamW(model.parameters(),
                                  lr=5e-4,
                                  weight_decay=cfg.WEIGHT_DECAY)

    # learning rate scheduler that reduces LR when validation loss plateaus
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,
                                                           mode='min',
                                                           factor=0.5,
                                                           patience=5)

    # variables for early stopping
    best_val_loss = float('inf')
    patience_counter = 0

    # training loop
    for epoch in range(cfg.NUM_EPOCHS):
        print(f"\nEpoch {epoch+1}/{cfg.NUM_EPOCHS}")

        # train for one epoch
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)


        val_loss = validate_epoch(model, val_loader, criterion, device)

        # adjust learning rate based on validation loss
        scheduler.step(val_loss)

        print(f"Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}")

        # check if this is the best model so far
        if val_loss < best_val_loss - cfg.MIN_DELTA:

            best_val_loss = val_loss
            patience_counter = 0

            torch.save(model.state_dict(), cfg.MODEL_SAVE_PATH)
            print(f"Model saved (Val Loss: {val_loss:.4f})")
        else:
            patience_counter += 1
            print(f"No improvement for {patience_counter} epochs")

        # early stopping check
        if patience_counter >= 20:
            print(f"Early stopping triggered at epoch {epoch+1}")
            break

    print(f"Training complete. Best model saved at: {cfg.MODEL_SAVE_PATH}")
    return model


def evaluate_test_set():
    """Run the trained model on the test set."""

    print("Evaluating test set")

    predictor = PupilIrisPredictor(cfg.MODEL_SAVE_PATH, device=device)

    results_df, metrics_df = predictor.predict_dataset(cfg.TEST_IMG_DIR,
                                                       cfg.TEST_GT_DIR,
                                                       output_csv='test_predictions.csv')

    return results_df, metrics_df


def visualize_prediction(img_path, pred_params):
    """Display the original image with predicted ellipses and masks."""

    # load image
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # make a copy to draw ellipses on
    img_pred = img.copy()

    # pupil green
    if pred_params['pupil'] is not None:
        pupil = pred_params['pupil']
        center = (int(pupil[0]), int(pupil[1]))
        axes = (int(pupil[2]), int(pupil[3]))
        angle = np.degrees(pupil[4])
        cv2.ellipse(img_pred, center, axes, angle,
                   startAngle=0, endAngle=360,
                   color=(0, 255, 0), thickness=2)

    # iris blue
    if pred_params['iris'] is not None:
        iris = pred_params['iris']
        center = (int(iris[0]), int(iris[1]))
        axes = (int(iris[2]), int(iris[3]))
        angle = np.degrees(iris[4])
        cv2.ellipse(img_pred, center, axes, angle,
                   startAngle=0, endAngle=360,
                   color=(0, 0, 255), thickness=2)

    # create colored mask visualization
    mask_viz = np.zeros_like(img)

    # color pupil pixels red
    if pred_params.get('pupil_mask') is not None:
        pupil_pixels = pred_params['pupil_mask'] == 1
        mask_viz[pupil_pixels] = [255, 0, 0]

    # color iris pixels cyan
    if pred_params.get('iris_mask') is not None:
        iris_pixels = pred_params['iris_mask'] == 1
        mask_viz[iris_pixels] = [0, 255, 255]

    # show all three images side by side
    plt.figure(figsize=(15, 5))

    plt.subplot(131)
    plt.imshow(img)
    plt.title('Original Image')
    plt.axis('off')

    plt.subplot(132)
    plt.imshow(img_pred)
    plt.title('Predicted Ellipses\nGreen: Pupil | Blue: Iris')
    plt.axis('off')

    plt.subplot(133)
    plt.imshow(mask_viz)
    plt.title('Segmentation Masks')
    plt.axis('off')

    plt.tight_layout()
    plt.show()


if __name__ == '__main__':
    """
    Example usage:

    Train the model:
        model = train_model()

    Evaluate on test set:
        results_df, metrics_df = evaluate_test_set()
    """
    print("Setup complete.")

model = train_model()

results_df, metrics_df = evaluate_test_set()

# Part 7: Metrics results
metrics_df.to_csv("test_metrics.csv", index=False)
print("Saved metrics to test_metrics.csv")

overall_metrics = {
    "Metric": ["Pupil Diameter Error (%)", "Iris Diameter Error (%)",
               "Pupil IoU (%)", "Iris IoU (%)"],
    "Value": [9.71, 2.74, 78.22, 91.79]
}


metrics_summary_df = pd.DataFrame(overall_metrics)


metrics_summary_df.to_csv("test_metrics_summary.csv", index=False)
print("Saved overall metrics to test_metrics_summary.csv")

"""**Below is a Script using Runner function to run on the test set.**"""

'''
from pathlib import Path
import torch

# Config
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

MODEL_PATH = "best_pupil_iris_model.pth"  # path to model
TEST_IMG_DIR =                            # insert test set
TEST_GT_DIR =                             # insert labels
OUTPUT_CSV =                              # name of CSV to save predictions

# Load Predictor
predictor = PupilIrisPredictor(MODEL_PATH, device=DEVICE)

# Run Predictions
# Returns:
# - results_df: dataframe with predictions for each image
# - metrics_df: dataframe with evaluation metrics (only if GT is provided)
results_df, metrics_df = predictor.predict_dataset(
    TEST_IMG_DIR,
    gt_dir=TEST_GT_DIR,
    output_csv=OUTPUT_CSV
)

print(f"Predictions saved to {OUTPUT_CSV}")
if TEST_GT_DIR:
    print(metrics_df)  # only prints if GT exists
'''

# Example

from pathlib import Path

model_path = '/content/best_pupil_iris_model.pth'
predictor = PupilIrisPredictor(model_path, device=device)


test_img = Path(cfg.TEST_IMG_DIR) / "03611_left.png"
pred = predictor.predict_single(test_img)

visualize_prediction(test_img, pred)